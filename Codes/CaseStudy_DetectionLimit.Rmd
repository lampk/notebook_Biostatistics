---
title: "Detection limit"
bibliography: refs.bib
output: 
  html_document:
    theme: cerulean
    toc: yes
    toc_float: yes
---

## Reading list

* Stephen Senn, Nick Holford, Hans Hockey (2012) The ghosts of departed quantities: approaches to dealing with observations below the limit of quantitation. Statistics in Medicine, 31: 4280–4295.

* https://discourse.datamethods.org/t/modelling-outcome-data-with-a-limit-of-detection-is-a-hurdle-model-appropriate/1427/7

* Beal  SJ.  Ways  to  fit  a  PK  model  with  some  data  below  the  quantification  limit. Journal of Pharmacokinetics and Pharmacodynamics 2001; 28 :481–504.

* Beal SL. Errata: ways to fit a PK model with some data below the quantification limit. Journal of Pharmacokinetics and Pharmacodynamics 2002; 29 :309.

## Summary

### Below detection limit (BDL)

* BDL is the problem when the measurements are reported as being below the quantification limit of the assay.

* Commonly seen in pharmacokinetic data.

* "quantification limit" (QL): the lowest concentration at which the assay has been validated

### Ways to deal with BDL [@Beal_2001]

## Notation

* Data $y_i, i = 1, \ldots, n$ are  a  simple  random  sample  from  a  normal  distribution  for  which  the  probability  that  a  given  random  value Y takes  on  the  observed  value y can  be represented by

$$ f(Y = y) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} (\frac{y - \mu}{\sigma})^2} $$

* $\phi(.)$ is the probability density function of a standard normal.

* $\Phi(.)$ is the cumulative density function of the standard normal.

* $c$ is the LLOQ value

* $m$ values are greater than or equal to $c$ --> $n - m$ are less than c and hence BLQ

* Assume that the first $m$ values are not BLQ and the remaining $n - m$ values are BLQ

* Probability that a given value is BLQ

$$ F(y) = \int_{-\infty}^c f(y)dy = \Phi(\frac{c - \mu}{\sigma}) $$
* If we do not actually observe any of the measurements but are simply informed of the values of $m$ and $n$, the likelihood is

$$ L_1(\mu, \sigma; y_1, \ldots, y_n) = \left[ \Phi(\frac{c - \mu}{\sigma}) \right]^{n - m} \left[1 - \Phi(\frac{c - \mu}{\sigma})\right]^m$$
* If we do not know how many observations are below the limit of quantitation but that we observe all values above the limit of quantitation, the likelihood is

$$ L_2(\mu, \sigma; y_1, \ldots, y_m) = \frac{\prod_{i = 1}^m \phi(\frac{y_i - \mu}{\sigma})}{[1 - \Phi(\frac{c - \mu}{\sigma})]^m} $$
* If we now know not only the values of all measurements above the LLOQ but also how many are below the LLOQ, the likelihood is

$$ L_3(\mu, \sigma; y_1, \ldots, y_n) = L_1 \times L_2 = \left[ \Phi(\frac{c - \mu}{\sigma}) \right]^{n - m} \prod_{i = 1}^m \phi(\frac{y_i - \mu}{\sigma})$$

#### M1: discard BDL observations

* simply discard them and apply least squares to the remaining observations

* Problem:
    + create bias as the lower of the remaining observations misrepresent the true lower concentrations (the lower remaining observations are selectively too high)
    
* Likelihood function



#### M2

*  the BQL observations can be discarded, and under the assumption that all the D ( t ) are normal, the method of maximum conditional likelihood estimation can be applied to the remaining observations (method M2)

* the likelihood for the data, conditional on the fact that by design, all (remaining) observations are above the QL, is maximized with respect to the model parameters

#### M3: handle the BQL observations as fixed-point (or Type I) censored observations

* "The maximum likelihood estimation method is used to fit the PK model to all the observations, and the likelihoods for the BQL observations in particular are taken to be the likelihoods that these observations are indeed BQL"

* allows the BQL observations to be retained, but handles these as censored observations, under the assumption that all the D ( t ) are normal.

* The likelihood for all the data is maximized with respect to the model parameters, and the likelihood for a BQL observation in particular is taken to be the likelihood that the observation is indeed BQL.

#### M4: M3 + adjustment to recognize that a measurement cannot really be negative

#### M5: impute with DL/2

#### M6

* each BQL observation x is replaced by QL/2, except that any and all consecutive BQL observations succeeding x are discarded

#### M7: impute with 0

* Problem:
    + create bias as 0 is always too low.
    
## Case studies

### Voriconazole data [@Senn_2012]

```{r}
## voriconazole data

### original concentration values
dat <- c(8505, 8489, 8024, 7893, 7284, 7180, 6417, 6286, 5956, 5771, 
         5602, 5478, 5354, 5324, 4887, 4240, 3614, 3611, 3484, 3402,
         3110, 2851, 2736, 2730, 2687, 2603, 2433, 2426, 2389, 2359,
         1981, 1952, 1821, 1771, 1699, 1530, 1528, 1526, 1409, 1290,
          965,  844,  715,  689,  516,  467,  231,  206,  164, 40.6
         )

### define censoring limit (Lower Limit Of Quantification)
LLOQ <- 1000

### assumed data are censored at the LLOQ
### - create censoring indicator
CENSOR <- I(dat < LLOQ) 
### - numeric version of censoring indicator
Ind <- as.numeric(CENSOR)
### - calculate censored values
COBS <- dat + (LLOQ - dat) * Ind
### - create data frame for analysis
vori <- data.frame(concentration = dat,
                   censor = CENSOR,
                   indicator = Ind,
                   values = COBS)


```

```{r}
## likelihood functions
LLF <- function(pars, type = c("L1", "L2", "L3"), c = 1000, y) {
  # extract parameters from the vector
  mu <- pars[1]
  sigma <- pars[2]
  
  # indicator of BLQ
  ind <- as.numeric(y <= c)
  # calculate likelihood
  if (type == "L1") {
    out <- sum(log((1 - pnorm((c - mu)/sigma))^(1 - ind) * (pnorm((c - mu)/sigma))^(ind)))
  } else {
    if (type == "L2") {
      out <- sum(log((dnorm((y - mu)/sigma)/(1 - pnorm((c - mu)/sigma)))^(1 - ind)))
    } else {
      L1 <- sum(log((1 - pnorm((c - mu)/sigma))^(1 - ind) * (pnorm((c - mu)/sigma))^(ind)))
      L2 <- sum(log((dnorm((y - mu)/sigma)/(1 - pnorm((c - mu)/sigma)))^(1 - ind)))
      out <- L1 + L2
    }
  }
  return(out)
}

LLF(pars = c(1000, 1000), type = "L1", c = 1000, y = vori$values)
```

```{r}
### Analysis using censReg package of Arne Henningsen
### - load library censReg
library(censReg)
### - Fit data
voriCR <- censReg(values ~ 1, left = LLOQ, data = vori)
### - Results of fitting data as censored
summary(voriCR)
### - Estimated parameter estimates
(estimate <- coef(voriCR))
### - calculate sigma from log-sigma
(sigma <- exp(estimate[2]))
### - extract SE of log-sigma
(se.logsigma <- summary(voriCR)$estimate[2, 2])
### - calculate SE of sigma based on delta method
(se.sigma <- sigma * se.logsigma)
### - so standard deviation is sigma
sigma
### - standard error of standard deviation
se.sigma
```


## References
